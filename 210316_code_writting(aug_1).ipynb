{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "union-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from math import cos, sin, pi\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incoming-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 이동\n",
    "os.chdir('data/1. open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elementary-terrorist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_df.csv',\n",
       " 'test_imgs.zip',\n",
       " 'test_imgs',\n",
       " 'submission',\n",
       " 'train_imgs.zip',\n",
       " 'train_imgs',\n",
       " 'left_right_flip',\n",
       " 'sample_submission.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wicked-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터 중 10%를 검증 데이터로 사용\n",
    "\n",
    "# csv 파일 불러오기\n",
    "data = pd.read_csv('train_df.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 경로 설정\n",
    "data_paths = sorted(glob.glob('./train_imgs/*.jpg'))\n",
    "test_paths = sorted(glob.glob('./test_imgs/*.jpg'))\n",
    "\n",
    "data['path'] = data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 길이는:  3776\n",
      "검증 데이터 길이는:  419\n"
     ]
    }
   ],
   "source": [
    "# 데이터 프레임 랜덤하게 분할\n",
    "\n",
    "# 전체 데이터 중 90%는 학습 데이터 활용\n",
    "train = data.sample(frac=0.9, random_state=2021)\n",
    "print('학습 데이터 길이는: ', len(train))\n",
    "\n",
    "# 전체 데이터 중 10%는 검증 데이터 활용\n",
    "valid = data.drop(train.index)\n",
    "print('검증 데이터 길이는: ', len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "based-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좌우 반전\n",
    "def left_right_flip(images, keypoints):\n",
    "    flipped_keypoints = []\n",
    "    flipped_images = np.flip(images, axis=1)\n",
    "    for idx, sample_keypoints in enumerate(keypoints):\n",
    "        if idx%2 == 0:\n",
    "            flipped_keypoints.append(480.-sample_keypoints)\n",
    "        else:\n",
    "            flipped_keypoints.append(sample_keypoints)\n",
    "    \n",
    "    # left_right_keypoints_convert\n",
    "    for i in range(8):\n",
    "        flipped_keypoints[2+(4*i):4+(4*i)], flipped_keypoints[4+(4*i):6+(4*i)] = flipped_keypoints[4+(4*i):6+(4*i)], flipped_keypoints[2+(4*i):4+(4*i)]\n",
    "    flipped_keypoints[36:38], flipped_keypoints[38:40] = flipped_keypoints[38:40], flipped_keypoints[36:38]\n",
    "    flipped_keypoints[44:46], flipped_keypoints[46:48] = flipped_keypoints[46:48], flipped_keypoints[44:46]\n",
    "    \n",
    "    return flipped_images, flipped_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "domestic-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation Setting\n",
    "pixel_shifts = [12]\n",
    "rotation_angles = [12]\n",
    "inc_brightness_ratio = 1.2\n",
    "dec_brightness_ratio = 0.8\n",
    "noise_ratio = 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "taken-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수직/수평 동시 이동\n",
    "# forloop에서 shift_x, shift_y 중 하나만 놓으면\n",
    "# 수직 또는 수평 이동만 따로 시행 가능\n",
    "def shift_images(images, keypoints):\n",
    "    # tensor -> numpy\n",
    "    images = images.numpy()\n",
    "    shifted_images = []\n",
    "    shifted_keypoints = []\n",
    "    for shift in pixel_shifts:   \n",
    "        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n",
    "            # 이동할 matrix 생성\n",
    "            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\n",
    "            shifted_keypoint = np.array([])\n",
    "            shifted_x_list = np.array([])\n",
    "            shifted_y_list = np.array([])\n",
    "            # 이미지 이동\n",
    "            shifted_image = cv2.warpAffine(images, M, (480,270), flags=cv2.INTER_CUBIC)\n",
    "            # 이동한만큼 keypoint 수정\n",
    "            for idx, point in enumerate(keypoints):\n",
    "                if idx%2 == 0: \n",
    "                    shifted_keypoint = np.append(shifted_keypoint, point+shift_x)\n",
    "                    shifted_x_list = np.append(shifted_x_list, point+shift_x)\n",
    "                else: \n",
    "                    shifted_keypoint =np.append(shifted_keypoint, point+shift_y)\n",
    "                    shifted_y_list = np.append(shifted_y_list, point+shift_y)\n",
    "            # 수정된 keypoint가 이미지 사이즈를 벗어나지 않으면 append\n",
    "            if np.all(0.0<shifted_x_list) and np.all(shifted_x_list<480) and np.all(0.0<shifted_y_list) and np.all(shifted_y_list<270):\n",
    "                shifted_images.append(shifted_image.reshape(270,480,3))\n",
    "                shifted_keypoints.append(shifted_keypoint)\n",
    "\n",
    "    return shifted_images, shifted_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blind-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 회전\n",
    "def rotate_augmentation(images, keypoints):\n",
    "    # tensor -> numpy\n",
    "    images = images.numpy()\n",
    "    rotated_images = []\n",
    "    rotated_keypoints = []\n",
    "    \n",
    "    for angle in rotation_angles:\n",
    "        for angle in [angle,-angle]:\n",
    "            # 회전할 matrix 생성\n",
    "            M = cv2.getRotationMatrix2D((240,135), angle, 1.0)\n",
    "            # cv2_imshow로는 문제없지만 추후 plt.imshow로 사진을 확인할 경우 black screen 생성...\n",
    "            # 혹시 몰라 matrix를 ndarray로 변환\n",
    "            M = np.array(M, dtype=np.float32)\n",
    "            angle_rad = -angle*pi/180\n",
    "            rotated_image = cv2.warpAffine(images, M, (480,270))\n",
    "            rotated_images.append(rotated_image)\n",
    "            \n",
    "            # keypoint를 copy하여 forloop상에서 값이 계속 없데이트 되는 것을 회피\n",
    "            rotated_keypoint = keypoints.copy()\n",
    "            rotated_keypoint[0::2] = rotated_keypoint[0::2] - 240\n",
    "            rotated_keypoint[1::2] = rotated_keypoint[1::2] - 135\n",
    "            \n",
    "            for idx in range(0,len(rotated_keypoint),2):\n",
    "                rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\n",
    "                rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\n",
    "\n",
    "            rotated_keypoint[0::2] = rotated_keypoint[0::2] + 240\n",
    "            rotated_keypoint[1::2] = rotated_keypoint[1::2] + 135\n",
    "            rotated_keypoints.append(rotated_keypoint)\n",
    "        \n",
    "    return rotated_images, rotated_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "atmospheric-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 해상도 조절\n",
    "def alter_brightness(images):\n",
    "    altered_brightness_images = []\n",
    "    inc_brightness_images = np.clip(images*inc_brightness_ratio, 0.0, 1.0)\n",
    "    dec_brightness_images = np.clip(images*dec_brightness_ratio, 0.0, 1.0)\n",
    "    altered_brightness_images.append(inc_brightness_images)\n",
    "    altered_brightness_images.append(dec_brightness_images)\n",
    "    return altered_brightness_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "little-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random 노이즈 추가\n",
    "def add_noise(images):\n",
    "    images = images.numpy()\n",
    "    noise = noise_ratio * np.random.randn(270,480,3)\n",
    "    noise = noise.astype(np.float32)\n",
    "    # 생성한 noise를 원본에 add\n",
    "    noisy_image = cv2.add(images, noise)\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "angry-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator():\n",
    "    # 원본 이미지 resize\n",
    "    for i in range(len(train)):\n",
    "        img = tf.io.read_file(train['path'][i]) # path(경로)를 통해 이미지 읽기\n",
    "        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\n",
    "        img = tf.image.resize(img, [270,480]) # 이미지 resize \n",
    "        img = img/255                         # 이미지 rescaling\n",
    "        target = train.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\n",
    "        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\n",
    "\n",
    "        yield (img, target)\n",
    "    \n",
    "    # horizontal flip\n",
    "#     for i in range(len(train)):\n",
    "#         img = tf.io.read_file(train['path'][i]) \n",
    "#         img = tf.image.decode_jpeg(img, channels=3) \n",
    "#         img = tf.image.resize(img, [270,480]) \n",
    "#         img = img/255\n",
    "#         target = train.iloc[:,1:49].iloc[i,:] \n",
    "#         target = target/4\n",
    "#         img, target = left_right_flip(img, target)\n",
    "        \n",
    "#         yield (img, target)\n",
    "\n",
    "    # Horizontal & Vertical shift\n",
    "#     for i in range(len(train)):\n",
    "#         img = tf.io.read_file(train['path'][i])\n",
    "#         img = tf.image.decode_jpeg(img, channels=3)\n",
    "#         img = tf.image.resize(img, [270,480])\n",
    "#         img = img/255\n",
    "#         target = train.iloc[:,1:49].iloc[i,:]\n",
    "#         target = target/4\n",
    "#         img_list, target_list = shift_images(img, target)\n",
    "#         for shifted_img, shifted_target in zip(img_list, target_list):\n",
    "            \n",
    "#             yield (shifted_img, shifted_target)\n",
    "\n",
    "    # Rotation\n",
    "    for i in range(len(train)):\n",
    "        img = tf.io.read_file(train['path'][i])\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [270,480])\n",
    "        img = img/255\n",
    "        target = train.iloc[:,1:49].iloc[i,:]\n",
    "        target = target/4\n",
    "        img_list, target_list = rotate_augmentation(img, target)\n",
    "        for rotated_img, rotated_target in zip(img_list, target_list):\n",
    "            \n",
    "            yield (rotated_img, rotated_target)\n",
    "\n",
    "    # Alter_Brightness\n",
    "    for i in range(len(train)):\n",
    "        img = tf.io.read_file(train['path'][i])\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [270,480])\n",
    "        img = img/255\n",
    "        target = train.iloc[:,1:49].iloc[i,:]\n",
    "        target = target/4\n",
    "        img_list = alter_brightness(img)\n",
    "        for altered_brightness_images in img_list:\n",
    "            \n",
    "            yield (altered_brightness_images, target)\n",
    "\n",
    "    # Adding_Noise\n",
    "#     for i in range(len(train)):\n",
    "#         img = tf.io.read_file(train['path'][i])\n",
    "#         img = tf.image.decode_jpeg(img, channels=3)\n",
    "#         img = tf.image.resize(img, [270,480])\n",
    "#         img = img/255\n",
    "#         target = train.iloc[:,1:49].iloc[i,:]\n",
    "#         target = target/4\n",
    "#         noisy_img = add_noise(img)\n",
    "\n",
    "#         yield (noisy_img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "underlying-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validGenerator():\n",
    "    # 원본 이미지 resize\n",
    "    for i in range(len(valid)):\n",
    "        img = tf.io.read_file(valid['path'][i]) # path(경로)를 통해 이미지 읽기\n",
    "        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\n",
    "        img = tf.image.resize(img, [270,480]) # 이미지 resize \n",
    "        img = img/255                         # 이미지 rescaling\n",
    "        target = valid.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\n",
    "        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\n",
    "\n",
    "        yield (img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "offensive-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(1)\n",
    "valid_dataset = tf.data.Dataset.from_generator(validGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "chief-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback 설정\n",
    "earlystop = EarlyStopping(patience=7)\n",
    "learning_rate_reduction=ReduceLROnPlateau(\n",
    "                        monitor= \"val_loss\", \n",
    "                        patience = 2, \n",
    "                        factor = 0.85, \n",
    "                        min_lr=1e-7,\n",
    "                        verbose=1)\n",
    "\n",
    "model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \n",
    "        filepath=\"./baseline_with_augmentation.h5\", #모델 파일 경로\n",
    "        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n",
    "        save_best_only=True)\n",
    "\n",
    "callbacks = [earlystop, learning_rate_reduction, model_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "latter-exhaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 270, 480, 32)      864       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 270, 480, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 270, 480, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 270, 480, 32)      9216      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 270, 480, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 270, 480, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 135, 240, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 135, 240, 64)      18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 135, 240, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 135, 240, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 135, 240, 64)      36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 135, 240, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 135, 240, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 67, 120, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 67, 120, 96)       55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 67, 120, 96)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 67, 120, 96)       384       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 67, 120, 96)       82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 67, 120, 96)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 67, 120, 96)       384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 33, 60, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 33, 60, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 33, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 33, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 33, 60, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 33, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 33, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 30, 256)       294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 30, 256)       589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 16, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 15, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 15, 512)        1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 15, 512)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 15, 512)        2048      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 15, 512)        2359296   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 8, 15, 512)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 15, 512)        2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 61440)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               31457792  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                24624     \n",
      "=================================================================\n",
      "Total params: 36,376,464\n",
      "Trainable params: 36,372,112\n",
      "Non-trainable params: 4,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Structure\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(270,480,3)))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(48))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controlled-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "valid.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "owned-protection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "590/590 [==============================] - 384s 651ms/step - loss: 1585.7030 - accuracy: 0.1237 - val_loss: 245.8639 - val_accuracy: 0.2721 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "590/590 [==============================] - 369s 625ms/step - loss: 878.6879 - accuracy: 0.1568 - val_loss: 246.4317 - val_accuracy: 0.4320 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "590/590 [==============================] - 367s 622ms/step - loss: 711.4460 - accuracy: 0.1849 - val_loss: 148.7606 - val_accuracy: 0.5394 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "590/590 [==============================] - 372s 631ms/step - loss: 612.2494 - accuracy: 0.2129 - val_loss: 201.6499 - val_accuracy: 0.4821 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "590/590 [==============================] - 378s 641ms/step - loss: 548.4604 - accuracy: 0.2171 - val_loss: 129.3766 - val_accuracy: 0.4487 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "590/590 [==============================] - 396s 671ms/step - loss: 524.1025 - accuracy: 0.2303 - val_loss: 200.5426 - val_accuracy: 0.5131 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "590/590 [==============================] - ETA: 0s - loss: 486.8396 - accuracy: 0.2544\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.499999785271939e-05.\n",
      "590/590 [==============================] - 364s 617ms/step - loss: 486.8396 - accuracy: 0.2544 - val_loss: 137.9189 - val_accuracy: 0.5227 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "590/590 [==============================] - 365s 619ms/step - loss: 451.5804 - accuracy: 0.2697 - val_loss: 117.0075 - val_accuracy: 0.4487 - lr: 8.5000e-05\n",
      "Epoch 9/10\n",
      "590/590 [==============================] - 394s 668ms/step - loss: 437.2349 - accuracy: 0.2828 - val_loss: 103.0313 - val_accuracy: 0.5656 - lr: 8.5000e-05\n",
      "Epoch 10/10\n",
      "590/590 [==============================] - 374s 634ms/step - loss: 426.4519 - accuracy: 0.2882 - val_loss: 126.3352 - val_accuracy: 0.4749 - lr: 8.5000e-05\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=10,\n",
    "                    validation_data=valid_dataset,\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "interim-contact",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:15<00:00, 105.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1600, 270, 480, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=[]\n",
    "\n",
    "for test_path in tqdm(test_paths):\n",
    "    img=tf.io.read_file(test_path)\n",
    "    img=tf.image.decode_jpeg(img, channels=3)\n",
    "    img=tf.image.resize(img, [270,480])\n",
    "    img=img/255\n",
    "    X_test.append(img)\n",
    "\n",
    "X_test=tf.stack(X_test, axis=0)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ordinary-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mental-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission.iloc[:,1:] = pred * 4     # image size를 1920x1080 -> 480x270으로 바꿔서 예측했으므로 * 4\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "combined-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('aug_rotation_bright.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
